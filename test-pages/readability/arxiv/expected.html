<div id="readability-page-1" class="page"><div id="content-inner">
    
    
  <div><p><span>Authors:</span><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Firooz,+H" rel="nofollow">Hamed Firooz</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Sanjabi,+M" rel="nofollow">Maziar Sanjabi</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Englhardt,+A" rel="nofollow">Adrian Englhardt</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Gupta,+A" rel="nofollow">Aman Gupta</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Levine,+B" rel="nofollow">Ben Levine</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Olgiati,+D" rel="nofollow">Dre Olgiati</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Polatkan,+G" rel="nofollow">Gungor Polatkan</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Melnychuk,+I" rel="nofollow">Iuliia Melnychuk</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ramgopal,+K" rel="nofollow">Karthik Ramgopal</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Talanine,+K" rel="nofollow">Kirill Talanine</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Kutta" rel="nofollow">Kutta Srinivasan</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Simon,+L" rel="nofollow">Luke Simon</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Sivasubramoniapillai,+N" rel="nofollow">Natesh Sivasubramoniapillai</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ayan,+N+F" rel="nofollow">Necip Fazil Ayan</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Song,+Q" rel="nofollow">Qingquan Song</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Samira" rel="nofollow">Samira Sriram</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ghosh,+S" rel="nofollow">Souvik Ghosh</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Song,+T" rel="nofollow">Tao Song</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Kothapalli,+V" rel="nofollow">Vignesh Kothapalli</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhai,+X" rel="nofollow">Xiaoling Zhai</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xu,+Y" rel="nofollow">Ya Xu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+Y" rel="nofollow">Yu Wang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Dai,+Y" rel="nofollow">Yun Dai</a></p></div>            
  <p><a href="http://fakehost/pdf/2501.16450">View PDF</a>
  <a href="https://arxiv.org/html/2501.16450v1">HTML (experimental)</a></p><blockquote>
          <span>Abstract:</span>Ranking and recommendation systems are the foundation for numerous online experiences, ranging from search results to personalized content delivery. These systems have evolved into complex, multilayered architectures that leverage vast datasets and often incorporate thousands of predictive models. The maintenance and enhancement of these models is a labor intensive process that requires extensive feature engineering. This approach not only exacerbates technical debt but also hampers innovation in extending these systems to emerging problem domains. In this report, we present our research to address these challenges by utilizing a large foundation model with a textual interface for ranking and recommendation tasks. We illustrate several key advantages of our approach: (1) a single model can manage multiple predictive tasks involved in ranking and recommendation, (2) decoder models with textual interface due to their comprehension of reasoning capabilities, can generalize to new recommendation surfaces and out-of-domain problems, and (3) by employing natural language interfaces for task definitions and verbalizing member behaviors and their social connections, we eliminate the need for feature engineering and the maintenance of complex directed acyclic graphs of model dependencies. We introduce our research pre-production model, 360Brew V1.0, a 150B parameter, decoder-only model that has been trained and fine-tuned on LinkedIn's data and tasks. This model is capable of solving over 30 predictive tasks across various segments of the LinkedIn platform, achieving performance levels comparable to or exceeding those of current production systems based on offline metrics, without task-specific fine-tuning. Notably, each of these tasks is conventionally addressed by dedicated models that have been developed and maintained over multiple years by teams of a similar or larger size than our own.
  </blockquote>

  
  <div>
    <table summary="Additional metadata"><tbody><tr>
        <td>Subjects:</td>
        <td>
          <span>Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI)</td>
      </tr><tr>
        <td>Cite as:</td>
        <td><span><a href="https://arxiv.org/abs/2501.16450">arXiv:2501.16450</a> [cs.IR]</span></td>
      </tr>
      <tr>
        <td>&nbsp;</td>
        <td>(or <span>
            <a href="https://arxiv.org/abs/2501.16450v1">arXiv:2501.16450v1</a> [cs.IR]</span> for this version)
        </td>
      </tr>
      <tr>
        <td>&nbsp;</td>
        <td>              <a href="https://doi.org/10.48550/arXiv.2501.16450" id="arxiv-doi-link">https://doi.org/10.48550/arXiv.2501.16450</a><div>
            <p><span></span>                  arXiv-issued DOI via DataCite</p>
          </div>
        </td>
      </tr></tbody></table>
  </div>
</div><div>
    <h2>Submission history</h2><p> From: Maziar Sanjabi [<a href="http://fakehost/show-email/fa91fd8b/2501.16450" rel="nofollow">view email</a>]      <br>    <strong>[v1]</strong>
      Mon, 27 Jan 2025 19:14:52 UTC (1,931 KB)<br>
</p></div></div>